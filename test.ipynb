{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import load_parser\n",
    "# import grammar_selector #la idea es importar la función del otro script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es para testear una oración cualquiera usando de gramática integral grammar\n",
    "sents = ['vos jugás']\n",
    "grammar = 'integral_grammar.fcfg'\n",
    "for results in nltk.interpret_sents(sents, grammar):\n",
    "    for (synrep, semrep) in results:\n",
    "             print(synrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La idea es que esto vaya a parar a grammar_selector.py\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import load_parser\n",
    "import re\n",
    "import os\n",
    "\n",
    "#############################\n",
    "# Chequeo de Léxico\n",
    "#############################\n",
    "\n",
    "# Esto es para que al momento de importar este archivo reuna en un solo \n",
    "# archivo todo el léxico. \n",
    "# Si el archivo ya existe no lo vuelve a armar, trabaja con el que está.\n",
    "# Estaría bueno repensar esto como una función para poder setear qué archivos\n",
    "# uno quiere que coleccione, para de esa manera hacerlo más portable por ejemplo\n",
    "# para otras lenguas.\n",
    "\n",
    "if not os.path.exists('./reglas_automaticas/integral_lexicon.fcfg'):\n",
    "    lexicon_files = os.listdir(path='./reglas_automaticas')\n",
    "    outfile = open('./reglas_automaticas/integral_lexicon.fcfg', 'w')\n",
    "    for fname in lexicon_files:\n",
    "        with open(f'./reglas_automaticas/{fname}', 'r') as infile:\n",
    "            for line in infile:\n",
    "               outfile.write(line)\n",
    "            \n",
    "#############################\n",
    "# Función para parsear \n",
    "#############################\n",
    "\n",
    "# Esta sería la función que arma la gramática personalizada para cada oración\n",
    "# En principio la pensé así (pero metan mano tranca):\n",
    "# grammar_selector toma tres argumentos: una gramática de base, un léxico y una oración.\n",
    "# 1) lo primero que hace es borrar integral_grammar.fcfg (la idea es usar siempre ese archivo\n",
    "#   una y otra vez cada vez que se parsea. \n",
    "# 2) Después crea integral_grammar.fcfg de vuelta y le escribe todas las reglas de la gramática base\n",
    "# 3) Después tokeniza la oración.\n",
    "# 4) Por cada palabra de la lista de tokens, la idea sería que si esa palabra está al final de línea\n",
    "#    entre comillas simples después de una flecha (no sé cuánto puede ser necesario especificar del \n",
    "#    contexto), agregue esa línea a integral_grammar\n",
    "# 5) Finalmente, parsea la oración usando como base integral_grammar.fcfg (o sea, la gramática de base\n",
    "#    más las reglas de reescritura que reescriben a los tokens que posee la oración).\n",
    "# \n",
    "# En principio, entiendo que el punto 1, 2, 3 y 5 están bien, pero tendrían que chequear bien el 4.\n",
    "\n",
    "def grammar_selector(grammar_base, lexicon, sentence):\n",
    "# punto 1\n",
    "    if os.path.exists('integral_grammar.fcfg'):\n",
    "        os.remove('integral_grammar.fcfg')\n",
    "# punto 2\n",
    "    with open('integral_grammar.fcfg', 'w') as f:\n",
    "        for line in grammar_base:\n",
    "            f.write(line)\n",
    "            #print(line)\n",
    "# punto 3\n",
    "    tokens = word_tokenize(sentence)\n",
    "    #print(tokens)\n",
    "# punto 4\n",
    "    for word in tokens:\n",
    "        for line in lexicon:\n",
    "            #print(word)\n",
    "            if line.__contains__(word): # con esto básicamente toma a word en cualquier lugar de la línea\n",
    "                # esto genera problemas porque para 'el chico', por ejemplo, selecciona cualquier línea en \n",
    "                # la que haya un 'el', como la que reescribe \"abellaca\" por ejempo.\n",
    "                with open(\"integral_grammar.fcfg\", \"a\") as f:\n",
    "                    f.write(line)\n",
    "                    print(line)\n",
    "# punto 5\n",
    "    for results in nltk.interpret_sents(sentence, open(\"integral_grammar.fcfg\", \"r\")):\n",
    "        for (synrep, semrep) in results:\n",
    "            print(synrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para testar grammar_selector\n",
    "\n",
    "integral_lexicon = open('reglas_automaticas/integral_lexicon.fcfg','r')\n",
    "base_grammar = open('GramaticaDeRasgosBase.txt', 'r')\n",
    "sent = \"el chico vino\"\n",
    "grammar_selector(base_grammar, integral_lexicon, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
